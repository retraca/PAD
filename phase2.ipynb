{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams \n",
    "from zipfile import ZipFile\n",
    "from kneebow.rotor import Rotor\n",
    "from kneed import KneeLocator\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 2 million word corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "archive = ZipFile('corpus2mw.zip', 'r')\n",
    "\n",
    "fileList = archive.namelist()\n",
    "for file in fileList:\n",
    "    corpus.append((archive.read(file)).decode('UTF-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process text (separate special characters from words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = re.compile('[\\w \\-]') \n",
    "\n",
    "def processText(corpus):\n",
    "    corp = []\n",
    "    for text in corpus:\n",
    "        listT = list(text)\n",
    "        i = 0\n",
    "        for  c in listT:\n",
    "            if (not regexp.search(c) and not listT[i-1]==' ') or not regexp.search(listT[i-1]) and regexp.search(c):\n",
    "                listT.insert(i, ' ')\n",
    "            i +=1\n",
    "        corp.append(''.join(listT))\n",
    "    return corp\n",
    "\n",
    "readPickle = False\n",
    "if readPickle:\n",
    "    with open('corpusList', 'rb') as fp:\n",
    "        corpus = pickle.load(fp)\n",
    "else: \n",
    "    corpus =  processText(corpus)\n",
    "\n",
    "    with open('corpusList', 'wb') as fp:\n",
    "            pickle.dump(corpus, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_freq_doc(text, minG, maxG):\n",
    "   freq_dist = FreqDist()\n",
    "   if len(text) > 1:\n",
    "       tokens = text.strip().split()\n",
    "       for i in range(minG, maxG+1):\n",
    "           grams = ngrams(tokens, i)\n",
    "           freq_dist.update(grams)\n",
    "\n",
    "   return dict(freq_dist)\n",
    "\n",
    "\n",
    "def compute_freq_corpus(minG, maxG):\n",
    "   freq_dist = FreqDist()\n",
    "   for text in corpus:\n",
    "        if len(text) > 1:\n",
    "            tokens = text.strip().split()\n",
    "            for i in range(minG, maxG+1):\n",
    "                grams = ngrams(tokens, i)\n",
    "                freq_dist.update(grams)\n",
    "\n",
    "   return dict(freq_dist)\n",
    "\n",
    "\n",
    "freq_dict = compute_freq_corpus(1, 8)\n",
    "filtered_dict = {' '.join(key):val for key, val in freq_dict.items() if val > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dict_sorted= sorted(filtered_dict.items(), key=lambda x: len(x[0].split()), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53190\n",
      "53303\n",
      "53186\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#unigrams\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\w+')\n",
    "vec_fit = vectorizer.fit_transform(corpus)\n",
    "single_word_list = vectorizer.get_feature_names()\n",
    "single_count_list = np.asarray(vec_fit.sum(axis=0))[0]\n",
    "single_freq_dict = dict(zip(single_word_list,single_count_list))\n",
    "\n",
    "single_freq_dict = {key:val for key, val in single_freq_dict.items() if val > 1}\n",
    "single_freq_dict = {k: v for k, v in sorted(single_freq_dict.items(), key=lambda item: item[1])}\n",
    "values = np.fromiter(single_freq_dict.values(), dtype=float)\n",
    "stop_words_list = np.stack((np.arange(0, len(single_freq_dict)), values), axis = -1)\n",
    "list_of_counts = list(single_freq_dict.items())\n",
    "\n",
    "\n",
    "rotor = Rotor()\n",
    "rotor.fit_rotate(stop_words_list)\n",
    "elbow_idx = rotor.get_elbow_index()\n",
    "print(elbow_idx)  \n",
    "#rotor.plot_elbow()\n",
    "\n",
    "kn = KneeLocator(stop_words_list[:,0] ,stop_words_list[:,1], curve='convex', direction='increasing')\n",
    "print(int(kn.knee))\n",
    "\n",
    "stop = 0\n",
    "deltaX = 200\n",
    "for idx, i, j in zip(range(0, len(values)), values, values[deltaX:]):\n",
    "    if((j-i)>stop): stop = idx\n",
    "print(stop)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.scatter(stop_words_list[:,0] ,stop_words_list[:,1] , s=1,c='blue', marker='.')\n",
    "ax.set_yscale('log')\n",
    "#ax.set_xscale('log')\n",
    "fig.set_size_inches(10, 7)\n",
    "plt.axvline(x=elbow_idx, color='r', linestyle='--')\n",
    "\n",
    "\n",
    "#relev_exp_unigrams = list_of_counts[:elbow_idx] #not really necessary\n",
    "stop_words_unigrams = list_of_counts[stop:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LocalMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "expressions_count={}\n",
    "\n",
    "for key, val in filtered_dict_sorted:\n",
    "    expressions_count[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=[]\n",
    "\n",
    "for key, val in stop_words_unigrams:\n",
    "    stop_words.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['east',\n",
       " 'age',\n",
       " 'located',\n",
       " 'because',\n",
       " '2009',\n",
       " 'like',\n",
       " 'long',\n",
       " 'march',\n",
       " 'found',\n",
       " 'home',\n",
       " 'september',\n",
       " 'line',\n",
       " '2011',\n",
       " 'station',\n",
       " '2008',\n",
       " 'end',\n",
       " 'international',\n",
       " 'system',\n",
       " 'band',\n",
       " 'if',\n",
       " '0',\n",
       " 'any',\n",
       " '2010',\n",
       " 'career',\n",
       " 'general',\n",
       " 'use',\n",
       " '7',\n",
       " 'won',\n",
       " 'town',\n",
       " '10',\n",
       " 'so',\n",
       " 'west',\n",
       " '000',\n",
       " 'now',\n",
       " 'same',\n",
       " 'based',\n",
       " 'day',\n",
       " 'game',\n",
       " 'house',\n",
       " 'population',\n",
       " 'government',\n",
       " 'company',\n",
       " 'several',\n",
       " '6',\n",
       " 'john',\n",
       " 'following',\n",
       " 'each',\n",
       " 'played',\n",
       " 'released',\n",
       " 'league',\n",
       " 'county',\n",
       " 'music',\n",
       " 'called',\n",
       " 'four',\n",
       " 'work',\n",
       " 'life',\n",
       " 'district',\n",
       " 'de',\n",
       " 'name',\n",
       " 'number',\n",
       " 'until',\n",
       " 'will',\n",
       " 'area',\n",
       " 'group',\n",
       " 'since',\n",
       " 'history',\n",
       " 'series',\n",
       " 'north',\n",
       " 'against',\n",
       " 'album',\n",
       " 'people',\n",
       " 'family',\n",
       " 'them',\n",
       " '4',\n",
       " 'south',\n",
       " 'early',\n",
       " 'including',\n",
       " 'born',\n",
       " 'however',\n",
       " 'high',\n",
       " 'through',\n",
       " 'both',\n",
       " 'film',\n",
       " 'before',\n",
       " 'states',\n",
       " 'became',\n",
       " 'war',\n",
       " 'well',\n",
       " '5',\n",
       " 'being',\n",
       " 'than',\n",
       " 'united',\n",
       " 'these',\n",
       " 'team',\n",
       " 'second',\n",
       " 'university',\n",
       " 'season',\n",
       " 'no',\n",
       " 'american',\n",
       " 'part',\n",
       " 'i',\n",
       " 'while',\n",
       " 'under',\n",
       " 'known',\n",
       " 'many',\n",
       " '3',\n",
       " 'made',\n",
       " 'can',\n",
       " 'then',\n",
       " 'between',\n",
       " 'where',\n",
       " 'later',\n",
       " 'about',\n",
       " 'used',\n",
       " 'state',\n",
       " 'him',\n",
       " 'city',\n",
       " 'such',\n",
       " 'national',\n",
       " 'three',\n",
       " 'some',\n",
       " 'world',\n",
       " 'out',\n",
       " 'over',\n",
       " 'would',\n",
       " 'year',\n",
       " 'up',\n",
       " 'most',\n",
       " 'years',\n",
       " 'may',\n",
       " '2',\n",
       " 'only',\n",
       " 'school',\n",
       " 'more',\n",
       " 'into',\n",
       " 'during',\n",
       " 'there',\n",
       " '1',\n",
       " 'time',\n",
       " 'all',\n",
       " 'when',\n",
       " 'other',\n",
       " 'been',\n",
       " 'she',\n",
       " 'her',\n",
       " 'have',\n",
       " 'two',\n",
       " 'who',\n",
       " 'not',\n",
       " 'they',\n",
       " 'new',\n",
       " 'after',\n",
       " 'its',\n",
       " 'but',\n",
       " 'one',\n",
       " 'their',\n",
       " 'first',\n",
       " 'had',\n",
       " 'has',\n",
       " 'or',\n",
       " 'be',\n",
       " 'also',\n",
       " 'this',\n",
       " 'curid',\n",
       " 'url',\n",
       " 'wiki',\n",
       " 'id',\n",
       " 'doc',\n",
       " 'wikipedia',\n",
       " 'org',\n",
       " 'en',\n",
       " 'http',\n",
       " 'title',\n",
       " 'which',\n",
       " 'are',\n",
       " 'were',\n",
       " 'an',\n",
       " 'it',\n",
       " 'his',\n",
       " 'from',\n",
       " 'at',\n",
       " 'that',\n",
       " 'he',\n",
       " 's',\n",
       " 'by',\n",
       " 'with',\n",
       " 'as',\n",
       " 'on',\n",
       " 'for',\n",
       " 'is',\n",
       " 'was',\n",
       " 'to',\n",
       " 'a',\n",
       " 'in',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "poss_re={}\n",
    "\n",
    "for key, val in filtered_dict_sorted:\n",
    "    words= key.split()\n",
    "    n= len(words)\n",
    "    if len(words) > 1:    \n",
    "        ownpref=''\n",
    "        ownsuf=''\n",
    "        for i in range(0,n):\n",
    "            if i==0:\n",
    "                ownpref += key.split(' ')[i]\n",
    "            elif i==n-1:\n",
    "                ownsuf += key.split(' ')[i]\n",
    "            else:\n",
    "                ownpref += ' ' + key.split(' ')[i]\n",
    "                ownsuf += key.split(' ')[i] + ' '\n",
    "        xpref= expressions_count[ownpref]\n",
    "        xsuf= expressions_count[ownsuf]\n",
    "        scpg= val**2 /(xpref * xsuf)\n",
    "        diceg = 2 * val / (xpref + xsuf)\n",
    "\n",
    "        poss_re[key] = {'n':n,  'freq': val, 'scpg': scpg, 'diceg': diceg,'xpref': ownpref, 'xsuf': ownsuf}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the United States Census\n",
      "added to the National Register\n",
      "19th and early 20th\n",
      "1950s and early 1960s\n",
      "Rio de Janeiro\n",
      "Atlantic Coast Conference\n",
      "Hall of Fame\n",
      "Bocas del Toro\n",
      "per square mile\n",
      "Hispanic or Latino\n",
      "above sea level\n",
      "Newfoundland and Labrador\n",
      "Major League Baseball\n",
      "Saturdays and Sundays\n",
      "Bosnia and Herzegovina\n",
      "Trinidad and Tobago\n",
      "25 to 44\n",
      "45 to 64\n",
      "Greater Poland Voivodeship\n",
      "Cirque du Soleil\n",
      "Buffy the Vampire\n",
      "Best Foreign Language\n",
      "Prince Edward Island\n",
      "mid to late\n",
      "Shivani and Mohini\n",
      "Pan American Games\n",
      "Australian Capital Territory\n",
      "Eurovision Song Contest\n",
      "except where noted\n",
      "Antigua and Barbuda\n",
      "Environmental Protection Agency\n",
      "Central African Republic\n",
      "above mean sea\n",
      "Jammu and Kashmir\n",
      "Chief Operating Officer\n",
      "Ho Chi Minh\n",
      "subtropical or tropical\n",
      "Talk â€˜ N\n",
      "leave of absence\n",
      "Los Angeles\n",
      "Nova Scotia\n",
      "Notre Dame\n",
      "Hong Kong\n",
      "hip hop\n",
      "alma mater\n",
      "Puerto Rico\n",
      "Las Vegas\n",
      "Buenos Aires\n",
      "Lok Sabha\n"
     ]
    }
   ],
   "source": [
    "points=['.',',','?','(',')','!','@','&','^','~','|','>','<', '%', '$', '[', ']', '{', '}', ':', ';', '-','a','s', '_', '+', '=', '*', '\\\\', '\\'', '\\\"', '`', '#', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '/', '-', \"'\", 't']\n",
    "relevant_expressions={}\n",
    "res=[]\n",
    "\n",
    "for key, val in poss_re.items():\n",
    "    if val['n'] > 2:\n",
    "        #print(val['n'], 'ngram')\n",
    "        \n",
    "        words = key.split()\n",
    "        if words[0].lower() in stop_words:\n",
    "            continue\n",
    "        if words[-1].lower() in stop_words:\n",
    "            continue\n",
    "        if val['freq'] <= 2:\n",
    "            continue\n",
    "        grams = key.split()\n",
    "        puntcount = 0\n",
    "        for g in grams:\n",
    "            if g in points:\n",
    "                puntcount+=1\n",
    "        if puntcount!=0:\n",
    "            continue\n",
    "        #best x\n",
    "        try:\n",
    "            xpref = poss_re[val['xpref']]['scpg']\n",
    "            xsuf = poss_re[val['xsuf']]['scpg']\n",
    "            bestxscp = max(xpref, xsuf)\n",
    "            \n",
    "            xpref = poss_re[val['xpref']]['diceg']\n",
    "            xsuf = poss_re[val['xsuf']]['diceg']\n",
    "            bestxdice = max(xpref, xsuf)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        #best y\n",
    "        bestyscp=1    \n",
    "        for keyy, valy in poss_re.items():\n",
    "            if key in keyy and valy['n'] == val['n']+1:\n",
    "                if valy['scpg']<bestyscp:\n",
    "                    bestyscp=valy['scpg']\n",
    "                    \n",
    "        bestydice=1    \n",
    "        for keyy, valy in poss_re.items():\n",
    "            if key in keyy and valy['n'] == val['n']+1:\n",
    "                if valy['diceg']<bestydice:\n",
    "                    bestydice=valy['diceg']\n",
    "\n",
    "        #see if is relevant\n",
    "        if (val['scpg']-0.6)>(bestxscp+bestyscp)/2 and (val['diceg']-0.6)>(bestxdice+bestydice)/2:\n",
    "            print(key)\n",
    "            relevant_expressions[key]={'n':val['n'], 'freq': val['freq'], 'scpg': val['scpg'], 'xscp':bestxscp, 'yscp':bestyscp, 'diceg': val['diceg'], 'xdice':bestxdice, 'ydice':bestydice}\n",
    "            res.append(key)\n",
    "    \n",
    "    else:\n",
    "        words = key.split()\n",
    "        if words[0].lower() in stop_words:\n",
    "            continue\n",
    "        if words[-1].lower() in stop_words:\n",
    "            continue\n",
    "        if val['freq'] <= 2:\n",
    "            continue\n",
    "        grams = key.split()\n",
    "        puntcount = 0\n",
    "        for g in grams:\n",
    "            if g in points:\n",
    "                puntcount+=1\n",
    "        if puntcount!=0:\n",
    "            continue\n",
    "        \n",
    "        #print(val['n'], 'bigram')\n",
    "        for keyy, valy in poss_re.items():\n",
    "            if key in keyy and valy['n'] == val['n']+1:\n",
    "                if valy['scpg']<bestyscp:\n",
    "                    bestyscp=valy['scpg']\n",
    "                    \n",
    "        bestydice=1    \n",
    "        for keyy, valy in poss_re.items():\n",
    "            if key in keyy and valy['n'] == val['n']+1:\n",
    "                if valy['diceg']<bestydice:\n",
    "                    bestydice=valy['diceg'] \n",
    "        \n",
    "        if (val['scpg']-0.6)>bestyscp and (val['diceg']-0.6)>bestydice:\n",
    "            #print(key)\n",
    "            relevant_expressions[key]={'n':val['n'], 'freq': val['freq'], 'scpg': val['scpg'], 'yscp':bestyscp, 'diceg': val['diceg'], 'ydice':bestydice}\n",
    "            res.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision')\n",
    "print(len(relevant_expressions)/len(poss_re))\n",
    "\n",
    "\n",
    "\n",
    "recallList = []\n",
    "for i in range(0,200):\n",
    "    recallList.append(random.choice(poss_re.keys()))\n",
    "countR = 0\n",
    "for k in recallList:\n",
    "    if k in relevant_expressions.keys():\n",
    "        countR += 1\n",
    "print('Recall')\n",
    "print(countR/200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('REList', 'wb') as fp:\n",
    "    pickle.dump(relevant_expressions, fp)\n",
    "   \n",
    "with open('REList', 'rb') as fp:\n",
    "    relevant_expressions = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit and Implicit Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_RE_in_doc(RE):\n",
    "    count = 0\n",
    "    for text in corpus:\n",
    "        if RE in text:\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "def freq(RE,doc):\n",
    "    freq_dict = compute_freq_doc(doc, len(RE.split()), len(RE.split()))\n",
    "    if len(RE.split()) > 1:\n",
    "        freq_dict = {' '.join(key):val for key, val in freq_dict.items()}\n",
    "    else:\n",
    "        freq_dict = {''.join(key):val for key, val in freq_dict.items()}\n",
    "    return freq_dict[RE]\n",
    "\n",
    "\n",
    "def tf_idf(RE, doc_idx):\n",
    "    doc = corpus[doc_idx]\n",
    "    \n",
    "    freq_RE = freq(RE,doc)\n",
    "    \n",
    "    return (freq_RE/len(doc.strip().split()))*math.log(len(corpus)/count_RE_in_doc(RE))\n",
    "\n",
    "def findWholeWord(w):\n",
    "    return re.compile(r'\\b({0})\\b'.format(re.escape(w))).search\n",
    "\n",
    "def calc_prob(word):\n",
    "    sum_p = 0\n",
    "    for doc in corpus:\n",
    "        if findWholeWord(word)(doc): \n",
    "            sum_p += freq(word, doc)/len(doc.strip().split())\n",
    "    return (1/len(corpus))*sum_p\n",
    "\n",
    "def calc_cov(A,B):\n",
    "    probA = calc_prob(A)\n",
    "    probB = calc_prob(B)\n",
    "    sum_p = 0\n",
    "    for doc in corpus:\n",
    "        if findWholeWord(A)(doc) and findWholeWord(B)(doc):\n",
    "            sum_p += (freq(A, doc)/len(doc.strip().split())-probA)*(freq(B, doc)/len(doc.strip().split())-probB)\n",
    "    \n",
    "    return (1/(len(corpus)-1))*sum_p\n",
    "\n",
    "def correlation(A,B):\n",
    "    return calc_cov(A, B)/(math.sqrt(calc_cov(A, A))*math.sqrt(calc_cov(B, B)))\n",
    "\n",
    "def get_distances(A,B,doc):\n",
    "    closest = 0\n",
    "    farthest = 0\n",
    "    \n",
    "    listA = A.split()\n",
    "    listB = B.split()\n",
    "    listDoc = doc.strip().split()\n",
    "    \n",
    "    idx_pos_A_1 = [ i for i in range(len(listDoc)) if listDoc[i] == listA[0] ]\n",
    "    idx_pos_A_2 = [ i for i in range(len(listDoc)) if listDoc[i] == listA[-1] ]\n",
    "    idx_pos_B_1 = [ i for i in range(len(listDoc)) if listDoc[i] == listB[0] ]\n",
    "    idx_pos_B_2 = [ i for i in range(len(listDoc)) if listDoc[i] == listB[-1] ]\n",
    "    \n",
    "    idx_pos_A_1_copy = idx_pos_A_1\n",
    "    idx_pos_A_2_copy = idx_pos_A_2\n",
    "    idx_pos_B_1_copy = idx_pos_B_1\n",
    "    idx_pos_B_2_copy = idx_pos_B_2\n",
    "    \n",
    "    for pos, idx in enumerate(idx_pos_A_1_copy):\n",
    "        for i, elem in enumerate(listA):\n",
    "            if listDoc[idx+i] != elem:\n",
    "                    idx_pos_A_1.pop(pos)\n",
    "                    break\n",
    "                \n",
    "    for pos, idx in enumerate(idx_pos_A_2_copy):\n",
    "        for i, elem in enumerate(reversed(listA)):\n",
    "            if listDoc[idx-i] != elem:\n",
    "                    idx_pos_A_2.pop(pos)  \n",
    "                    break\n",
    "                           \n",
    "    for pos, idx in enumerate(idx_pos_B_1_copy):\n",
    "        for i, elem in enumerate(listB):\n",
    "            if listDoc[idx+i] != elem:\n",
    "                    idx_pos_B_1.pop(pos) \n",
    "                    break\n",
    "            \n",
    "                \n",
    "    for pos, idx in enumerate(idx_pos_B_2_copy):\n",
    "        for i, elem in enumerate(reversed(listB)):\n",
    "            if listDoc[idx-i] != elem:\n",
    "                    idx_pos_B_2.pop(pos)    \n",
    "                    break\n",
    "    \n",
    "    listF = []\n",
    "    \n",
    "    for a in idx_pos_A_1:\n",
    "        for b in idx_pos_B_2:\n",
    "            listF.append(a-b)\n",
    "    for a in idx_pos_A_2:\n",
    "        for b in idx_pos_B_1:\n",
    "            listF.append(b-a)\n",
    "    listF = [ i for i in listF if i > 0 ]\n",
    "    \n",
    "    if len(listF) == 0: return 1\n",
    "\n",
    "    return min(listF)/max(listF)\n",
    "  \n",
    "def IP(A,B):\n",
    "    count = 0\n",
    "    sum_dist = 0 \n",
    "    for i, doc in enumerate(corpus):\n",
    "        if findWholeWord(A)(doc) and findWholeWord(B)(doc):\n",
    "            count += 1\n",
    "            sum_dist += get_distances(A, B, doc)\n",
    "        \n",
    "    return 1-(1/count)*sum_dist\n",
    "        \n",
    "       \n",
    "def sem_prox(A,B):\n",
    "    return correlation(A, B)*math.sqrt(IP(A,B))\n",
    "\n",
    "def occur_in_any_doc(A,B):\n",
    "    for doc in corpus:\n",
    "        if findWholeWord(A)(doc) and findWholeWord(B)(doc):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calc_comp_medio(RE):\n",
    "    count = 0\n",
    "    listRE = RE.split()\n",
    "    for w in listRE:\n",
    "        count += len(w)\n",
    "    return count/len(listRE)\n",
    "    \n",
    "def score_explicit(doc_idx):\n",
    "    doc = corpus[doc_idx]\n",
    "   \n",
    "    #unigrams\n",
    "    uni_dict = compute_freq_doc(doc, 1, 1)\n",
    "    uni_dict = {''.join(key):val for key, val in uni_dict.items()}\n",
    "\n",
    "    for k in uni_dict.keys():\n",
    "        uni_dict[k] = tf_idf(k,doc_idx) * calc_comp_medio(k)\n",
    "    top5_uni = dict(sorted(uni_dict.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "    #relevant expressions\n",
    "    re_dict = {}\n",
    "    for k in relevant_expressions.keys():\n",
    "        if k in doc:\n",
    "            re_dict[k] = relevant_expressions[k]['freq']\n",
    "    \n",
    "    \n",
    "    for k in re_dict.keys():\n",
    "        re_dict[k] = tf_idf(k,doc_idx) * calc_comp_medio(k)\n",
    "    top5_re = dict(sorted(re_dict.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "    \n",
    "    return top5_uni , top5_re if len(re_dict) != 0 else 'No REs in the document'\n",
    "    \n",
    "def score_implicit(doc_idx):\n",
    "    doc = corpus[doc_idx]\n",
    "    #relevant expressions\n",
    "    re_dict_in = {}\n",
    "    re_dict_out = {}\n",
    "    for k in relevant_expressions.keys():\n",
    "        if k in doc:\n",
    "            re_dict_in[k] = relevant_expressions[k]['freq']\n",
    "        else:\n",
    "            re_dict_out[k] = relevant_expressions[k]['freq']\n",
    "    \n",
    "    if len(re_dict_in) == 0: return 'No REs in the document'\n",
    "    \n",
    "    for k in re_dict_in.keys():\n",
    "        re_dict_in[k] = tf_idf(k,doc_idx) * calc_comp_medio(k)\n",
    "    top10_re = dict(sorted(re_dict_in.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "    \n",
    "    scores = {}\n",
    "    for k in re_dict_out.keys():\n",
    "        score = 0\n",
    "        for i, v in enumerate(top10_re):\n",
    "            if occur_in_any_doc(k, v):\n",
    "                score += sem_prox(k, v)/(i+1)\n",
    "        scores[k] = score\n",
    "    top10_scores = dict(sorted(scores.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "    return top10_scores\n",
    "\n",
    "#check document length\n",
    "len_docs = {}\n",
    "for i, doc in enumerate(corpus):\n",
    "    len_docs[i] = len(doc.strip().split())\n",
    "    \n",
    "\n",
    "#print(sorted(len_docs.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'PricewaterhouseCoopers': 0.22796157691837088, 'integrative': 0.22796157691837088, 'Randolph': 0.22393010018965329, 'qigong': 0.2159411323537057, 'criticizing': 0.1919951031822125}, 'No REs in the document')\n",
      "No REs in the document\n"
     ]
    }
   ],
   "source": [
    "print(\"Doc 303\") \n",
    "print(score_explicit(303))    \n",
    "print(score_implicit(303))\n",
    "\n",
    "print(\"Doc 1104\")\n",
    "print(score_explicit(1104))    \n",
    "print(score_implicit(1104))\n",
    "\n",
    "print(\"Doc 1230\") \n",
    "print(score_explicit(1230))    \n",
    "print(score_implicit(1230))\n",
    "\n",
    "print(\"Doc 1595\")\n",
    "print(score_explicit(1595))    \n",
    "print(score_implicit(1595)) \n",
    "\n",
    "print(\"Doc 2120\")\n",
    "print(score_explicit(2120))    \n",
    "print(score_implicit(2120))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a33b5d8c6c3b7e02d81a4ad58d04800e39ade03a10b1f4e7d06c5e533ec75f77"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
