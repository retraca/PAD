{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams \n",
    "from zipfile import ZipFile\n",
    "from kneebow.rotor import Rotor\n",
    "from kneed import KneeLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 2 million word corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "archive = ZipFile('corpus2mw.zip', 'r')\n",
    "\n",
    "fileList = archive.namelist()\n",
    "for file in fileList:\n",
    "    corpus.append((archive.read(file)).decode('UTF-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process text (separate special characters from words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = re.compile('[\\w \\-]') \n",
    "\n",
    "def processText(corpus):\n",
    "    corp = []\n",
    "    for text in corpus:\n",
    "        listT = list(text)\n",
    "        i = 0\n",
    "        for  c in listT:\n",
    "            if (not regexp.search(c) and not listT[i-1]==' ') or not regexp.search(listT[i-1]) and regexp.search(c):\n",
    "                listT.insert(i, ' ')\n",
    "            i +=1\n",
    "        corp.append(''.join(listT))\n",
    "    return corp\n",
    "\n",
    "readPickle = False\n",
    "if readPickle:\n",
    "    with open('corpusList', 'rb') as fp:\n",
    "        corpus = pickle.load(fp)\n",
    "else: \n",
    "    corpus =  processText(corpus)\n",
    "\n",
    "    with open('corpusList', 'wb') as fp:\n",
    "            pickle.dump(corpus, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_freq_doc(text, minG, maxG):\n",
    "   freq_dist = FreqDist()\n",
    "   if len(text) > 1:\n",
    "       tokens = text.strip().split()\n",
    "       for i in range(minG, maxG+1):\n",
    "           grams = ngrams(tokens, i)\n",
    "           freq_dist.update(grams)\n",
    "\n",
    "   return dict(freq_dist)\n",
    "\n",
    "\n",
    "def compute_freq_corpus(minG, maxG):\n",
    "   freq_dist = FreqDist()\n",
    "   for text in corpus:\n",
    "        if len(text) > 1:\n",
    "            tokens = text.strip().split()\n",
    "            for i in range(minG, maxG+1):\n",
    "                grams = ngrams(tokens, i)\n",
    "                freq_dist.update(grams)\n",
    "\n",
    "   return dict(freq_dist)\n",
    "\n",
    "\n",
    "freq_dict = compute_freq_corpus(1, 8)\n",
    "filtered_dict = {' '.join(key):val for key, val in freq_dict.items() if val > 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LocalMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(freq, pref_freqs, suff_freqs):\n",
    "    pref_freqs = list(pref_freqs)\n",
    "    suff_freqs = list(suff_freqs)\n",
    "    return 2 * freq / (sum(pref_freqs) / len(pref_freqs) + sum(suff_freqs) / len(suff_freqs))\n",
    "\n",
    "\n",
    "def scp(freq, pref_freqs, suff_freqs):\n",
    "    multiplied_freqs = [pref_freq * suff_freq for pref_freq, suff_freq in zip(pref_freqs, suff_freqs)]\n",
    "    return freq ** 2 * len(multiplied_freqs) / sum(multiplied_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relev_expressions={ }\n",
    "\n",
    "\n",
    "for key, value in filtered_dict.items():\n",
    "    n= len(key.split(' '))\n",
    "    if n>3:\n",
    "        own_freq = value/(len(corpus)-n)\n",
    "        #print(key)\n",
    "        ownpref=''\n",
    "        ownsuf=''\n",
    "        for i in range(0,n):\n",
    "            if i==0:\n",
    "                ownpref += key.split(' ')[i]\n",
    "            elif i==n-1:\n",
    "                ownsuf += key.split(' ')[i]\n",
    "            else:\n",
    "                ownpref += ' ' + key.split(' ')[i]\n",
    "                ownsuf += key.split(' ')[i] + ' '\n",
    "        #print(ownpref)\n",
    "        #print(ownsuf)\n",
    "        own_pref_freq= filtered_dict[ownpref]/(len(corpus)-(n-1))\n",
    "        own_suff_freq= filtered_dict[ownsuf]/(len(corpus)-(n-1))\n",
    "        gluew= own_freq**2 /(own_pref_freq * own_suff_freq)\n",
    "        #print(gluew)\n",
    "\n",
    "        if own_pref_freq>own_suff_freq:\n",
    "            aux= ownpref\n",
    "            auxfreqy= own_pref_freq\n",
    "        else:\n",
    "            aux= ownsuf\n",
    "            auxfreqy= own_suff_freq\n",
    "            \n",
    "        prefs=''\n",
    "        sufs=''\n",
    "        for i in aux.split(' ')[:-1]:\n",
    "            prefs+=i+' '\n",
    "        for i in aux.split(' ')[1:]:\n",
    "            sufs+=i+' '\n",
    "        \n",
    "        own_pref_freq= filtered_dict[prefs.strip()]/(len(corpus)-(n-2))\n",
    "        own_suff_freq= filtered_dict[sufs.strip()]/(len(corpus)-(n-2)) \n",
    "       \n",
    "        gluex= auxfreqy**2 /(own_pref_freq * own_suff_freq)\n",
    "        #print('x', gluex)\n",
    "        \n",
    "        besty=''\n",
    "        bestfreqy=0\n",
    "        for cha in filtered_dict.keys():\n",
    "            if key.split(' ')[:]==(cha.split(' ')[:-1] or cha.split(' ')[1:]):\n",
    "                #print(key, cha)\n",
    "                wy= filtered_dict[cha]/(len(corpus)-(n+1))\n",
    "                if wy>bestfreqy:\n",
    "                    bestfreqy=wy\n",
    "                    besty=cha\n",
    "                          \n",
    "        if(bestfreqy>0):\n",
    "            pref=''\n",
    "            suf=''\n",
    "            for i in besty.split(' ')[:-1]:\n",
    "                pref+=i+' '\n",
    "            for i in besty.split(' ')[1:]:\n",
    "                suf+=i+' '  \n",
    "                        \n",
    "            own_pref_freq= filtered_dict[pref.strip()]/(len(corpus)-(n-2))\n",
    "            own_suff_freq= filtered_dict[suf.strip()]/(len(corpus)-(n-2)) \n",
    "        \n",
    "            gluey= bestfreqy**2 /(own_pref_freq * own_suff_freq)\n",
    "            \n",
    "            re= (gluex+gluey)/2\n",
    "            #print('re', re)\n",
    "            #print('gluew', gluew)\n",
    "            if re<gluew:\n",
    "                relev_expressions[key]=re\n",
    "                print(key)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#unigrams\n",
    "single_freq_dict = {key:val for key, val in filtered_dict.items() if len(key.split()) == 1}\n",
    "#sort unigrams by value\n",
    "single_freq_dict = {k: v for k, v in sorted(single_freq_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "\n",
    "values = np.fromiter(single_freq_dict.values(), dtype=float)\n",
    "stop_words_list = np.stack((np.arange(0, len(single_freq_dict)), values), axis = -1)\n",
    "list_of_counts = list(single_freq_dict.items())\n",
    "\n",
    "\n",
    "rotor = Rotor()\n",
    "rotor.fit_rotate(stop_words_list)\n",
    "elbow_idx = rotor.get_elbow_index()\n",
    "print(elbow_idx)  \n",
    "#rotor.plot_elbow()\n",
    "\n",
    "kn = KneeLocator(stop_words_list[:,0] ,stop_words_list[:,1], curve='convex', direction='increasing')\n",
    "print(int(kn.knee))\n",
    "\n",
    "stop = 0\n",
    "deltaX = 200\n",
    "for idx, i, j in zip(range(0, len(values)), values, values[deltaX:]):\n",
    "    if((j-i)>stop): stop = idx\n",
    "print(stop)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.scatter(stop_words_list[:,0] ,stop_words_list[:,1] , s=1,c='blue', marker='.')\n",
    "ax.set_yscale('log')\n",
    "#ax.set_xscale('log')\n",
    "fig.set_size_inches(10, 7)\n",
    "plt.axvline(x=elbow_idx, color='r', linestyle='--')\n",
    "\n",
    "\n",
    "relev_exp_unigrams = list_of_counts[:elbow_idx] #not really necessary\n",
    "stop_words_unigrams = list_of_counts[elbow_idx:]\n",
    "\n",
    "#TODO filter relevant expression dictionary if w1 or wn in stop_words_unigrams -> delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit and Implicit Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_RE_in_doc(RE):\n",
    "    count = 0\n",
    "    for text in corpus:\n",
    "        if RE in text:\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "def freq(RE,doc):\n",
    "    freq_dict = compute_freq_doc(doc, len(RE.split()), len(RE.split()))\n",
    "    \n",
    "    return freq_dict[RE]\n",
    "\n",
    "\n",
    "def tf_idf(RE, doc_idx):\n",
    "    doc = corpus(idx)\n",
    "    \n",
    "    freq_RE = freq(RE,doc)\n",
    "    \n",
    "    return (freq_RE/len(doc))*math.log(len(corpus)/count_RE_in_doc(RE))\n",
    "\n",
    "def calc_prob(word):\n",
    "    sum_p = 0\n",
    "    for doc in corpus:\n",
    "        sum_p += freq(word, doc)/len(doc.split())\n",
    "    return (1/len(corpus))*sum_p\n",
    "\n",
    "def calc_cov(A,B):\n",
    "    probA = calc_prob(A)\n",
    "    probB = calc_prob(B)\n",
    "    sum_p = 0\n",
    "    for doc in corpus:\n",
    "        sum_p += (freq(A, doc)/len(doc.split())-probA)*(freq(B, doc)/len(doc.split())-probB)\n",
    "    return (1/len(corpus)-1)*sum_p\n",
    "\n",
    "def correlation(A,B):\n",
    "    return calc_cov(A, B)/(math.sqrt(calc_cov(A, A))*(math.sqrt(calc_cov(B, B))))\n",
    "\n",
    "\n",
    "  \n",
    "def IP(A,B):\n",
    "    count = 0\n",
    "    sum_dist = 0 \n",
    "    for doc in corpus:\n",
    "        if A in doc and B in doc:\n",
    "            count += 1\n",
    "            \n",
    "        \n",
    "        #TODO sum_dist += calculate closest divided by farthest distances between A and B\n",
    "        \n",
    "    \n",
    "    return 1-(1/count)*sum_dist\n",
    "        \n",
    "       \n",
    "def sem_prox(A,B):\n",
    "    return correlation(A, B)*math.sqrt(IP(A,B))\n",
    "\n",
    "def score_implicit(RE,doc):\n",
    "    return #TODO"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a33b5d8c6c3b7e02d81a4ad58d04800e39ade03a10b1f4e7d06c5e533ec75f77"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
